<h1 align="center">  Various-Attention-mechanisms </h1>
This repository contain various types of attention mechanism like Bahdanau , Soft attention , Additive Attention , Hierarchical Attention etc in Pytorch & Tensorflow, Keras


<p align="center">
  <img width="650" src="/Images/ Bahdanau_attention.png">
</p>

### Papers, research and study
|      Research Paper                 | Python Code  |
| :-------------------- | :----------: |
| [Paper ](https://arxiv.org/pdf/1409.0473.pdf) | [Code ](https://github.com/monk1337/Various-Attention-mechanisms/blob/master/2.0-%20Bahdanau_attention.py) |


Luong attention and Bahdanau attention



<img src="/Images/white.png" alt="My cool logo"/>

<img src="/Images/attention-mechanisms.png" alt="My cool logo"/>
 
<img src="/Images/alignments.png" alt="My cool logo"/>

#### working on an attetion module for tensorflow where you can just import the attention, check it out and contribute :

https://github.com/monk1337/Tensorflow-Attention-mechanisms

#Images source : http://cnyah.com/2017/08/01/attention-variants/
